{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trigger Finder",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aije775I6BtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecef0593-5592-4e0c-c382-4d03f44dfc6a"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from numpy import zeros"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyPy6wgE67QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Language:\n",
        "  def __init__(self):\n",
        "    token_list = [\"This\", \"is\", \"my\", \"sentence\"]\n",
        "    self.max_length = self.get_max_length(token_list)\n",
        "    sentences= [' '.join(tokens) for tokens in token_list]\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab =  len(tokenizer.word_index) + 1\n",
        "    self.word2vec = self.get_word2vec(\"/content/glove.twitter.27B.100d.txt\")\n",
        "    self.embedding_matrix = self.get_embedding_matrix()\n",
        "  \n",
        "  def get_max_length(self, token_list):\n",
        "    max_len = 0\n",
        "    for idx, tokens in enumerate(token_list):\n",
        "      if len(tokens) > max_len:\n",
        "        max_len = len(tokens)\n",
        "    return max_len\n",
        "  \n",
        "  def dump(self, data, filepath):\n",
        "    with open(filepath, \"wb\") as f:\n",
        "      pickle.dump(data, f)\n",
        "\n",
        "\n",
        "  def get_word2vec(self, file_path):\n",
        "    file_w2v = \"/content/w2v_100.pkl\"\n",
        "    exists = os.path.isfile(file_w2v)\n",
        "    if exists:                                      # load a prexisting pickel module\n",
        "      word2vec = pickle.load(open(file_w2v,\"rb\"))\n",
        "      return word2vec\n",
        "    else:\n",
        "      file = open(file_path, \"r\",encoding=\"utf8\")\n",
        "      if (file):\n",
        "        word2vec = dict()\n",
        "        split = file.read().splitlines()\n",
        "        for line in split:\n",
        "          key = line.split(' ', 1)[0]  # the first word is the key\n",
        "          value = np.array([float(val) for val in line.split(' ')[1:]])\n",
        "          word2vec[key] = value\n",
        "        self.dump(word2vec, os.path.join(\"w2v.pkl\"))\n",
        "        print(\"dumped the w2v file\")\n",
        "\n",
        "  def get_embedding_matrix(self):\n",
        "    embedding_matrix = zeros((self.vocab,25))\n",
        "    for word, i in self.tokenizer.word_index.items():\n",
        "      embedding_vector = self.word2vec.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "      return embedding_matrix\n",
        "\n",
        "  def get_encoded_data(self,tokens):\n",
        "    sentences = [' '.join(tokens) for tokens in tokens]\n",
        "\t\t# integer encode the documents\n",
        "    encoded_docs = self.tokenizer.texts_to_sequences(sentences)\n",
        "    # pad documents to a max length of 4 words\n",
        "    lng_data = pad_sequences(encoded_docs, maxlen=self.max_length, padding='post')\n",
        "    return lng_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4lYKfIoEAdt",
        "colab_type": "code",
        "outputId": "6fe7da25-2479-4906-e65e-71e1081dfefa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "language = Language()\n",
        "tokens = [\"This\", \"is\", \"my\", \"sentence\"]\n",
        "print(language.get_encoded_data(tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-af0eac1106f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"This\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoded_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0b9d0cf05611>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/glove.twitter.27B.100d.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0b9d0cf05611>\u001b[0m in \u001b[0;36mget_word2vec\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/glove.twitter.27B.100d.txt'"
          ]
        }
      ]
    }
  ]
}